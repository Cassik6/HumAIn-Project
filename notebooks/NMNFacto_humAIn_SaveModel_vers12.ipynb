{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Negative Matric Factorization\n",
    "\n",
    "Let's repeat thet opic modeling task from the previous lecture, but this time, we will use NMF instead of LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: loading data\n",
    "\n",
    "We will be using articles scraped from NPR (National Public Radio), obtained from their website [www.npr.org](http://www.npr.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christopheschellinck/Documents/Projects/project_NLP_humain/ipynb_files\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('..')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../news_data.json\", \"r\") as f:\n",
    "    papers10 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = pd.json_normalize(papers10[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>authors</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>None</td>\n",
       "      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>2020-02-05T17:08:34.343Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>AI may help create more sustainable data centres</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>None</td>\n",
       "      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>2020-02-05T17:08:34.355Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10815</td>\n",
       "      <td>Why a potential trillion dollar B2B bots indus...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "      <td>None</td>\n",
       "      <td>next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.365Z</td>\n",
       "      <td>2020-02-05T17:08:34.365Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10816</td>\n",
       "      <td>Why companies investing in AI today should exp...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "      <td>None</td>\n",
       "      <td>ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.375Z</td>\n",
       "      <td>2020-02-05T17:08:34.375Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10817</td>\n",
       "      <td>Tencent gears up for greater GPU acceleration ...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "      <td>None</td>\n",
       "      <td>gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "      <td>https://artificialintelligence-news.com/2017/0...</td>\n",
       "      <td>AInews</td>\n",
       "      <td>2020-02-05T17:08:34.385Z</td>\n",
       "      <td>2020-02-05T17:08:34.385Z</td>\n",
       "      <td>James Bourne</td>\n",
       "      <td>2017-04-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "2  10815  Why a potential trillion dollar B2B bots indus...   \n",
       "3  10816  Why companies investing in AI today should exp...   \n",
       "4  10817  Tencent gears up for greater GPU acceleration ...   \n",
       "\n",
       "                                             summary authors  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...    None   \n",
       "1  Enterprise data centre provider Aegis Data arg...    None   \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...    None   \n",
       "3  Organisations investing in artificial intellig...    None   \n",
       "4  Tencent’s cloud computing services will be bee...    None   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "2  next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...   \n",
       "3  ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...   \n",
       "4  gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
       "3  Organisations investing in artificial intellig...   \n",
       "4  Tencent’s cloud computing services will be bee...   \n",
       "\n",
       "                                                 url  source  \\\n",
       "0  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "1  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "2  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "3  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "4  https://artificialintelligence-news.com/2017/0...  AInews   \n",
       "\n",
       "                 created_at                updated_at        author  \\\n",
       "0  2020-02-05T17:08:34.343Z  2020-02-05T17:08:34.343Z  James Bourne   \n",
       "1  2020-02-05T17:08:34.355Z  2020-02-05T17:08:34.355Z  James Bourne   \n",
       "2  2020-02-05T17:08:34.365Z  2020-02-05T17:08:34.365Z  James Bourne   \n",
       "3  2020-02-05T17:08:34.375Z  2020-02-05T17:08:34.375Z  James Bourne   \n",
       "4  2020-02-05T17:08:34.385Z  2020-02-05T17:08:34.385Z  James Bourne   \n",
       "\n",
       "         date  \n",
       "0  2017-04-25  \n",
       "1  2017-04-25  \n",
       "2  2017-04-25  \n",
       "3  2017-04-25  \n",
       "4  2017-04-26  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'summary', 'authors', 'tags', 'text', 'url', 'source',\n",
       "       'created_at', 'updated_at', 'author', 'date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 12)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246    The UK government has announced the opening of...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.sample(1).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we don't have the topic of the articles! Let's use LDA to attempt to figure out clusters of the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers1 = papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>AI may help create more sustainable data centres</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10815</td>\n",
       "      <td>Why a potential trillion dollar B2B bots indus...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "      <td>next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10816</td>\n",
       "      <td>Why companies investing in AI today should exp...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "      <td>ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10817</td>\n",
       "      <td>Tencent gears up for greater GPU acceleration ...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "      <td>gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "2  10815  Why a potential trillion dollar B2B bots indus...   \n",
       "3  10816  Why companies investing in AI today should exp...   \n",
       "4  10817  Tencent gears up for greater GPU acceleration ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
       "3  Organisations investing in artificial intellig...   \n",
       "4  Tencent’s cloud computing services will be bee...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "2  next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...   \n",
       "3  ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...   \n",
       "4  gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...   \n",
       "\n",
       "                                                text  \n",
       "0  Cybersecurity provider ZingBox has announced t...  \n",
       "1  Enterprise data centre provider Aegis Data arg...  \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...  \n",
       "3  Organisations investing in artificial intellig...  \n",
       "4  Tencent’s cloud computing services will be bee...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers2 = papers1.drop(columns=['authors', 'url', 'source', 'created_at', 'updated_at', 'author', 'date'], axis=1, inplace = False)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1626, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    cybersecurity provider zingbox has announced t...\n",
       "1    enterprise data centre provider aegis data arg...\n",
       "2    from domino’s pizza to uber to bank of america...\n",
       "3    organisations investing in artificial intellig...\n",
       "4    tencent’s cloud computing services will be bee...\n",
       "Name: text_preprocessed, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers2['text_preprocessed'] = \\\n",
    "papers2['text'].map(lambda x: re.sub('\\s+', ' ', x))\n",
    "\n",
    "papers2['text_preprocessed'] = \\\n",
    "papers2['text_preprocessed'].map(lambda x: re.sub('[\\n]', ' ', x))\n",
    "\n",
    "papers2['text_preprocessed'] = \\\n",
    "papers2['text_preprocessed'].map(lambda x: re.sub('[\\']', '', x))\n",
    "\n",
    "papers2['text_preprocessed'] = \\\n",
    "papers2['text_preprocessed'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers2['text_preprocessed'] = \\\n",
    "papers2['text_preprocessed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers2['text_preprocessed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports:\n",
    "import spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy preparation for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en_core_web_sm = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing_article(line):\n",
    "    \n",
    "    string = ''\n",
    "    list1 = []\n",
    "    doc = nlp_en_core_web_sm(line)\n",
    "    for token in doc:\n",
    "        #string = ''.join(token.lemma_)\n",
    "        list1.append(token.lemma_)\n",
    "    \n",
    "    return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers2['text_lemmatized'] = \\\n",
    "papers2['text_preprocessed'].apply(lambda x: lemmatizing_article(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                              title  \\\n",
      "0   10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
      "1   10814   AI may help create more sustainable data centres   \n",
      "2   10815  Why a potential trillion dollar B2B bots indus...   \n",
      "3   10816  Why companies investing in AI today should exp...   \n",
      "4   10817  Tencent gears up for greater GPU acceleration ...   \n",
      "5   10818  Bonsai launches Early Access Program to help e...   \n",
      "6   10819  AI falls on the final furlong in predicting Ke...   \n",
      "7   10820  Most Britons want AI to support at least part ...   \n",
      "8   10821  Medicine, law and IT may be affected by the ri...   \n",
      "9   10822  Cisco acquires AI firm MindMeld to create more...   \n",
      "10  10823  University of Cambridge bolsters AI research e...   \n",
      "11  10824  Cray launches two new CS-Storm accelerated clu...   \n",
      "12  10825  UNICEF joins Apple, Google, Facebook et al in ...   \n",
      "13  10826  New intelligent street light software aims to ...   \n",
      "14  10827  Cylance launches first claimed AI-driven endpo...   \n",
      "15  10828  People.ai grabs $7 million funding to launch w...   \n",
      "16  10829  Here’s how AI can assist medical science in te...   \n",
      "17  10830  CrowdFlower raises $20 million in venture fund...   \n",
      "18  10831  CognitiveScale raises $50 million to bring in ...   \n",
      "19  10832  IEEE examines prospects of ‘generation AI’ thr...   \n",
      "\n",
      "                                              summary  \\\n",
      "0   Cybersecurity provider ZingBox has announced t...   \n",
      "1   Enterprise data centre provider Aegis Data arg...   \n",
      "2   From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
      "3   Organisations investing in artificial intellig...   \n",
      "4   Tencent’s cloud computing services will be bee...   \n",
      "5   US-based Bonsai is set to engage enterprises a...   \n",
      "6   The Kentucky Derby, one of the three races whi...   \n",
      "7   A new survey commissioned by the UC EXPO event...   \n",
      "8   Gartner has given a tentative guideline of 202...   \n",
      "9   Cisco has announced its intent to acquire Mind...   \n",
      "10  The Leverhulme Centre for the Future of Intell...   \n",
      "11  Cray has launched two new CS-Storm accelerated...   \n",
      "12  UNICEF has announced it has joined Partnership...   \n",
      "13  TCS Digital Software & Solutions Group, part o...   \n",
      "14  Australia-based Cylance has announced the gene...   \n",
      "15  People.ai has received $7 million in Series A ...   \n",
      "16  Artificial intelligence (AI) and deep learning...   \n",
      "17  San Francisco-based crowdsourcing firm CrowdFl...   \n",
      "18  CognitiveScale has announced it has raised an ...   \n",
      "19  A new report from the IEEE has shed light on w...   \n",
      "\n",
      "                                                 tags  \\\n",
      "0   device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
      "1   data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
      "2   next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...   \n",
      "3   ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...   \n",
      "4   gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...   \n",
      "5   early\\naccess\\nprogram\\nbonsai\\nbusiness\\nmach...   \n",
      "6   company\\nhorses\\nswarm\\nai\\ntechnology\\nunanim...   \n",
      "7   data\\nai\\ntechnology\\nuk\\nlike\\nexpo\\npolled\\n...   \n",
      "8   levels\\nai\\nlike\\ncost\\nutilities\\nprentice\\no...   \n",
      "9        conversational\\nnatural\\nconversation\\nwrote   \n",
      "10                                           ai\\nexpo   \n",
      "11  learning\\ncray\\nper\\ncluster\\nsystems\\nstorm\\n...   \n",
      "12                       data\\nindustry\\nunicef\\nexpo   \n",
      "13  led\\nstreetlights\\nstreetlight\\nintelligent\\ni...   \n",
      "14  endpoint\\ndata\\ncloud\\nthreats\\nresponse\\nthre...   \n",
      "15  insights\\npeople\\nsales\\nhelps\\nmake\\nleaders\\...   \n",
      "16                        diseases\\nmedical\\nai\\nexpo   \n",
      "17  machine\\nlearning\\ndata\\nindustry\\nventures\\na...   \n",
      "18  cognitivescale\\nindustry\\nhelp\\nai\\nhuman\\nexp...   \n",
      "19  generation\\nai\\nmillennial\\nparents\\nshowed\\ne...   \n",
      "\n",
      "                                                 text  \\\n",
      "0   Cybersecurity provider ZingBox has announced t...   \n",
      "1   Enterprise data centre provider Aegis Data arg...   \n",
      "2   From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
      "3   Organisations investing in artificial intellig...   \n",
      "4   Tencent’s cloud computing services will be bee...   \n",
      "5   US-based Bonsai is set to engage enterprises a...   \n",
      "6   The Kentucky Derby, one of the three races whi...   \n",
      "7   A new survey commissioned by the UC EXPO event...   \n",
      "8   Gartner has given a tentative guideline of 202...   \n",
      "9   Cisco has announced its intent to acquire Mind...   \n",
      "10  The Leverhulme Centre for the Future of Intell...   \n",
      "11  Cray has launched two new CS-Storm accelerated...   \n",
      "12  UNICEF has announced it has joined Partnership...   \n",
      "13  TCS Digital Software & Solutions Group, part o...   \n",
      "14  Australia-based Cylance has announced the gene...   \n",
      "15  People.ai has received $7 million in Series A ...   \n",
      "16  Artificial intelligence (AI) and deep learning...   \n",
      "17  San Francisco-based crowdsourcing firm CrowdFl...   \n",
      "18  CognitiveScale has announced it has raised an ...   \n",
      "19  A new report from the IEEE has shed light on w...   \n",
      "\n",
      "                                    text_preprocessed  \\\n",
      "0   cybersecurity provider zingbox has announced t...   \n",
      "1   enterprise data centre provider aegis data arg...   \n",
      "2   from domino’s pizza to uber to bank of america...   \n",
      "3   organisations investing in artificial intellig...   \n",
      "4   tencent’s cloud computing services will be bee...   \n",
      "5   us-based bonsai is set to engage enterprises a...   \n",
      "6   the kentucky derby one of the three races whic...   \n",
      "7   a new survey commissioned by the uc expo event...   \n",
      "8   gartner has given a tentative guideline of 202...   \n",
      "9   cisco has announced its intent to acquire mind...   \n",
      "10  the leverhulme centre for the future of intell...   \n",
      "11  cray has launched two new cs-storm accelerated...   \n",
      "12  unicef has announced it has joined partnership...   \n",
      "13  tcs digital software & solutions group part of...   \n",
      "14  australia-based cylance has announced the gene...   \n",
      "15  peopleai has received $7 million in series a f...   \n",
      "16  artificial intelligence (ai) and deep learning...   \n",
      "17  san francisco-based crowdsourcing firm crowdfl...   \n",
      "18  cognitivescale has announced it has raised an ...   \n",
      "19  a new report from the ieee has shed light on w...   \n",
      "\n",
      "                                      text_lemmatized  \n",
      "0   [cybersecurity, provider, zingbox, have, annou...  \n",
      "1   [enterprise, data, centre, provider, aegis, da...  \n",
      "2   [from, domino, ’s, pizza, to, uber, to, bank, ...  \n",
      "3   [organisation, invest, in, artificial, intelli...  \n",
      "4   [tencent, ’s, cloud, computing, service, will,...  \n",
      "5   [-PRON-, -, base, bonsai, be, set, to, engage,...  \n",
      "6   [the, kentucky, derby, one, of, the, three, ra...  \n",
      "7   [a, new, survey, commission, by, the, uc, expo...  \n",
      "8   [gartner, have, give, a, tentative, guideline,...  \n",
      "9   [cisco, have, announce, -PRON-, intent, to, ac...  \n",
      "10  [the, leverhulme, centre, for, the, future, of...  \n",
      "11  [cray, have, launch, two, new, cs, -, storm, a...  \n",
      "12  [unicef, have, announce, -PRON-, have, join, p...  \n",
      "13  [tcs, digital, software, &, solutions, group, ...  \n",
      "14  [australia, -, base, cylance, have, announce, ...  \n",
      "15  [peopleai, have, receive, $, 7, million, in, s...  \n",
      "16  [artificial, intelligence, (, ai, ), and, deep...  \n",
      "17  [san, francisco, -, base, crowdsource, firm, c...  \n",
      "18  [cognitivescale, have, announce, -PRON-, have,...  \n",
      "19  [a, new, report, from, the, ieee, have, shed, ...  \n"
     ]
    }
   ],
   "source": [
    "print(papers2.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert each line of a dataset column from list to string\n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \" \" \n",
    "    \n",
    "    # return string   \n",
    "    return (str1.join(s)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers2['text_lemmatized_string'] = \\\n",
    "papers2['text_lemmatized'].apply(lambda x: listToString(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                              title  \\\n",
      "0   10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
      "1   10814   AI may help create more sustainable data centres   \n",
      "2   10815  Why a potential trillion dollar B2B bots indus...   \n",
      "3   10816  Why companies investing in AI today should exp...   \n",
      "4   10817  Tencent gears up for greater GPU acceleration ...   \n",
      "5   10818  Bonsai launches Early Access Program to help e...   \n",
      "6   10819  AI falls on the final furlong in predicting Ke...   \n",
      "7   10820  Most Britons want AI to support at least part ...   \n",
      "8   10821  Medicine, law and IT may be affected by the ri...   \n",
      "9   10822  Cisco acquires AI firm MindMeld to create more...   \n",
      "10  10823  University of Cambridge bolsters AI research e...   \n",
      "11  10824  Cray launches two new CS-Storm accelerated clu...   \n",
      "12  10825  UNICEF joins Apple, Google, Facebook et al in ...   \n",
      "13  10826  New intelligent street light software aims to ...   \n",
      "14  10827  Cylance launches first claimed AI-driven endpo...   \n",
      "15  10828  People.ai grabs $7 million funding to launch w...   \n",
      "16  10829  Here’s how AI can assist medical science in te...   \n",
      "17  10830  CrowdFlower raises $20 million in venture fund...   \n",
      "18  10831  CognitiveScale raises $50 million to bring in ...   \n",
      "19  10832  IEEE examines prospects of ‘generation AI’ thr...   \n",
      "\n",
      "                                              summary  \\\n",
      "0   Cybersecurity provider ZingBox has announced t...   \n",
      "1   Enterprise data centre provider Aegis Data arg...   \n",
      "2   From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
      "3   Organisations investing in artificial intellig...   \n",
      "4   Tencent’s cloud computing services will be bee...   \n",
      "5   US-based Bonsai is set to engage enterprises a...   \n",
      "6   The Kentucky Derby, one of the three races whi...   \n",
      "7   A new survey commissioned by the UC EXPO event...   \n",
      "8   Gartner has given a tentative guideline of 202...   \n",
      "9   Cisco has announced its intent to acquire Mind...   \n",
      "10  The Leverhulme Centre for the Future of Intell...   \n",
      "11  Cray has launched two new CS-Storm accelerated...   \n",
      "12  UNICEF has announced it has joined Partnership...   \n",
      "13  TCS Digital Software & Solutions Group, part o...   \n",
      "14  Australia-based Cylance has announced the gene...   \n",
      "15  People.ai has received $7 million in Series A ...   \n",
      "16  Artificial intelligence (AI) and deep learning...   \n",
      "17  San Francisco-based crowdsourcing firm CrowdFl...   \n",
      "18  CognitiveScale has announced it has raised an ...   \n",
      "19  A new report from the IEEE has shed light on w...   \n",
      "\n",
      "                                                 tags  \\\n",
      "0   device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
      "1   data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
      "2   next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...   \n",
      "3   ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...   \n",
      "4   gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...   \n",
      "5   early\\naccess\\nprogram\\nbonsai\\nbusiness\\nmach...   \n",
      "6   company\\nhorses\\nswarm\\nai\\ntechnology\\nunanim...   \n",
      "7   data\\nai\\ntechnology\\nuk\\nlike\\nexpo\\npolled\\n...   \n",
      "8   levels\\nai\\nlike\\ncost\\nutilities\\nprentice\\no...   \n",
      "9        conversational\\nnatural\\nconversation\\nwrote   \n",
      "10                                           ai\\nexpo   \n",
      "11  learning\\ncray\\nper\\ncluster\\nsystems\\nstorm\\n...   \n",
      "12                       data\\nindustry\\nunicef\\nexpo   \n",
      "13  led\\nstreetlights\\nstreetlight\\nintelligent\\ni...   \n",
      "14  endpoint\\ndata\\ncloud\\nthreats\\nresponse\\nthre...   \n",
      "15  insights\\npeople\\nsales\\nhelps\\nmake\\nleaders\\...   \n",
      "16                        diseases\\nmedical\\nai\\nexpo   \n",
      "17  machine\\nlearning\\ndata\\nindustry\\nventures\\na...   \n",
      "18  cognitivescale\\nindustry\\nhelp\\nai\\nhuman\\nexp...   \n",
      "19  generation\\nai\\nmillennial\\nparents\\nshowed\\ne...   \n",
      "\n",
      "                                                 text  \\\n",
      "0   Cybersecurity provider ZingBox has announced t...   \n",
      "1   Enterprise data centre provider Aegis Data arg...   \n",
      "2   From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
      "3   Organisations investing in artificial intellig...   \n",
      "4   Tencent’s cloud computing services will be bee...   \n",
      "5   US-based Bonsai is set to engage enterprises a...   \n",
      "6   The Kentucky Derby, one of the three races whi...   \n",
      "7   A new survey commissioned by the UC EXPO event...   \n",
      "8   Gartner has given a tentative guideline of 202...   \n",
      "9   Cisco has announced its intent to acquire Mind...   \n",
      "10  The Leverhulme Centre for the Future of Intell...   \n",
      "11  Cray has launched two new CS-Storm accelerated...   \n",
      "12  UNICEF has announced it has joined Partnership...   \n",
      "13  TCS Digital Software & Solutions Group, part o...   \n",
      "14  Australia-based Cylance has announced the gene...   \n",
      "15  People.ai has received $7 million in Series A ...   \n",
      "16  Artificial intelligence (AI) and deep learning...   \n",
      "17  San Francisco-based crowdsourcing firm CrowdFl...   \n",
      "18  CognitiveScale has announced it has raised an ...   \n",
      "19  A new report from the IEEE has shed light on w...   \n",
      "\n",
      "                                    text_preprocessed  \\\n",
      "0   cybersecurity provider zingbox has announced t...   \n",
      "1   enterprise data centre provider aegis data arg...   \n",
      "2   from domino’s pizza to uber to bank of america...   \n",
      "3   organisations investing in artificial intellig...   \n",
      "4   tencent’s cloud computing services will be bee...   \n",
      "5   us-based bonsai is set to engage enterprises a...   \n",
      "6   the kentucky derby one of the three races whic...   \n",
      "7   a new survey commissioned by the uc expo event...   \n",
      "8   gartner has given a tentative guideline of 202...   \n",
      "9   cisco has announced its intent to acquire mind...   \n",
      "10  the leverhulme centre for the future of intell...   \n",
      "11  cray has launched two new cs-storm accelerated...   \n",
      "12  unicef has announced it has joined partnership...   \n",
      "13  tcs digital software & solutions group part of...   \n",
      "14  australia-based cylance has announced the gene...   \n",
      "15  peopleai has received $7 million in series a f...   \n",
      "16  artificial intelligence (ai) and deep learning...   \n",
      "17  san francisco-based crowdsourcing firm crowdfl...   \n",
      "18  cognitivescale has announced it has raised an ...   \n",
      "19  a new report from the ieee has shed light on w...   \n",
      "\n",
      "                                      text_lemmatized  \\\n",
      "0   [cybersecurity, provider, zingbox, have, annou...   \n",
      "1   [enterprise, data, centre, provider, aegis, da...   \n",
      "2   [from, domino, ’s, pizza, to, uber, to, bank, ...   \n",
      "3   [organisation, invest, in, artificial, intelli...   \n",
      "4   [tencent, ’s, cloud, computing, service, will,...   \n",
      "5   [-PRON-, -, base, bonsai, be, set, to, engage,...   \n",
      "6   [the, kentucky, derby, one, of, the, three, ra...   \n",
      "7   [a, new, survey, commission, by, the, uc, expo...   \n",
      "8   [gartner, have, give, a, tentative, guideline,...   \n",
      "9   [cisco, have, announce, -PRON-, intent, to, ac...   \n",
      "10  [the, leverhulme, centre, for, the, future, of...   \n",
      "11  [cray, have, launch, two, new, cs, -, storm, a...   \n",
      "12  [unicef, have, announce, -PRON-, have, join, p...   \n",
      "13  [tcs, digital, software, &, solutions, group, ...   \n",
      "14  [australia, -, base, cylance, have, announce, ...   \n",
      "15  [peopleai, have, receive, $, 7, million, in, s...   \n",
      "16  [artificial, intelligence, (, ai, ), and, deep...   \n",
      "17  [san, francisco, -, base, crowdsource, firm, c...   \n",
      "18  [cognitivescale, have, announce, -PRON-, have,...   \n",
      "19  [a, new, report, from, the, ieee, have, shed, ...   \n",
      "\n",
      "                               text_lemmatized_string  \n",
      "0   cybersecurity provider zingbox have announce t...  \n",
      "1   enterprise data centre provider aegis data arg...  \n",
      "2   from domino ’s pizza to uber to bank of americ...  \n",
      "3   organisation invest in artificial intelligence...  \n",
      "4   tencent ’s cloud computing service will be bee...  \n",
      "5   -PRON- - base bonsai be set to engage enterpri...  \n",
      "6   the kentucky derby one of the three race which...  \n",
      "7   a new survey commission by the uc expo event h...  \n",
      "8   gartner have give a tentative guideline of 202...  \n",
      "9   cisco have announce -PRON- intent to acquire m...  \n",
      "10  the leverhulme centre for the future of intell...  \n",
      "11  cray have launch two new cs - storm accelerate...  \n",
      "12  unicef have announce -PRON- have join partners...  \n",
      "13  tcs digital software & solutions group part of...  \n",
      "14  australia - base cylance have announce the gen...  \n",
      "15  peopleai have receive $ 7 million in series a ...  \n",
      "16  artificial intelligence ( ai ) and deep learni...  \n",
      "17  san francisco - base crowdsource firm crowdflo...  \n",
      "18  cognitivescale have announce -PRON- have raise...  \n",
      "19  a new report from the ieee have shed light on ...  \n"
     ]
    }
   ],
   "source": [
    "print(papers2.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy preparation for removing stopwords (automatic removed by spaCy as well as the manual selection of stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP WORD set in order to add manually stopwords to the stop word list\n",
    "STOP_WORDS |= {\"ai\", \"artificial\", \"intelligence\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_stopwords(line):\n",
    "    #  \"nlp\" Object is used to create documents with linguistic annotations.\n",
    "    my_doc = nlp(line)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = []\n",
    "    \n",
    "    for token in my_doc:\n",
    "        token_list.append(token.text)\n",
    "\n",
    "\n",
    "    # Create list of word tokens after removing stopwords\n",
    "    filtered_sentence =[] \n",
    "\n",
    "    for word in token_list: \n",
    "\n",
    "        lexeme = nlp.vocab[word]\n",
    "   \n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "            \n",
    "            \n",
    "    return filtered_sentence\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers2['text_cleaned'] = \\\n",
    "papers2['text_lemmatized_string'].apply(lambda x: removing_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [cybersecurity, provider, zingbox, announce, l...\n",
      "1    [enterprise, data, centre, provider, aegis, da...\n",
      "2    [domino, pizza, uber, bank, america, bots, hot...\n",
      "3    [organisation, invest, (, ), anticipate, 39, %...\n",
      "4    [tencent, cloud, computing, service, beef, gpu...\n",
      "5    [-PRON-, -, base, bonsai, set, engage, enterpr...\n",
      "6    [kentucky, derby, race, triple, crown, horse, ...\n",
      "7    [new, survey, commission, uc, expo, event, rev...\n",
      "8    [gartner, tentative, guideline, 2022, smart, m...\n",
      "9    [cisco, announce, -PRON-, intent, acquire, min...\n",
      "Name: text_cleaned, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(papers2['text_cleaned'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers2['text_cleaned_string'] = \\\n",
    "papers2['text_cleaned'].apply(lambda x: listToString(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     cybersecurity provider zingbox announce launch...\n",
      "1     enterprise data centre provider aegis data arg...\n",
      "2     domino pizza uber bank america bots hot proper...\n",
      "3     organisation invest ( ) anticipate 39 % revenu...\n",
      "4     tencent cloud computing service beef gpu accel...\n",
      "5     -PRON- - base bonsai set engage enterprise ind...\n",
      "6     kentucky derby race triple crown horse racing ...\n",
      "7     new survey commission uc expo event reveal 85 ...\n",
      "8     gartner tentative guideline 2022 smart machine...\n",
      "9     cisco announce -PRON- intent acquire mindmeld ...\n",
      "10    leverhulme centre future ( cfi ) join - - prof...\n",
      "11    cray launch new cs - storm accelerate cluster ...\n",
      "12    unicef announce -PRON- join partnership ( ) da...\n",
      "13    tcs digital software & solutions group tata co...\n",
      "14    australia - base cylance announce general avai...\n",
      "15    peopleai receive $ 7 million series funding le...\n",
      "16    ( ) deep learning help analyse image patient o...\n",
      "17    san francisco - base crowdsource firm crowdflo...\n",
      "18    cognitivescale announce -PRON- raise additiona...\n",
      "19    new report ieee shed light -PRON- ' generation...\n",
      "Name: text_cleaned_string, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(papers2['text_cleaned_string'].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Splitting the articles with a training part and a test part \n",
    "### Is to be doen now because after tfidf application not possible anymore to add the colum \"topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_train, articles_test = train_test_split(papers2, test_size = 0.25)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Preprocessing with TfidfVectorizer and fit_transform on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`max_df`**` : float in range [0.0, 1.0] or int, default=1.0`<br>\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "**`min_df`**` : float in range [0.0, 1.0] or int, default=1`<br>\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.b:  Applying fit_transform on the training data by TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_train = tfidf.fit_transform(articles_train['text_cleaned_string'])\n",
    "dtm_train = tfidf.fit_transform(articles_train['text_cleaned_string'])\n",
    "# dtm_train = tfidf.fit_transform(articles_train['text_preprocessed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1219x12369 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 281356 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 12369)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <p style=\"color:purple\">Step 5: NMF making the model with the training part of the data (THIS IS THE MODEL) </h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.a: Making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fit based on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(n_components=20, random_state=42)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This can take awhile, we're dealing with a large amount of documents!\n",
    "nmf_model.fit(dtm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.b: Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_model_NLP.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(nmf_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.c: Displaying Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12369"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indecipherable\n",
      "greene\n",
      "empower\n",
      "evan\n",
      "consortium\n",
      "richness\n",
      "norvig\n",
      "filmmaking\n",
      "uploaded\n",
      "sprawl\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0, len(tfidf.get_feature_names()))\n",
    "    print(tfidf.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alizadeh\n",
      "dublin\n",
      "conventional\n",
      "lago\n",
      "bonding\n",
      "knowingly\n",
      "skip\n",
      "supersede\n",
      "reskilling\n",
      "lehman\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    random_word_id = random.randint(0, len(tfidf.get_feature_names()))\n",
    "    print(tfidf.get_feature_names()[random_word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0007206 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00600381],\n",
       "       [0.        , 0.00152527, 0.        , ..., 0.        , 0.        ,\n",
       "        0.00158674],\n",
       "       [0.00593661, 0.        , 0.00126903, ..., 0.        , 0.00012436,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.0007598 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12369"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nmf_model.components_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_topic = nmf_model.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6184, 9203, 4840, ...,  677, 2957, 7209])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the indices that would sort this array.\n",
    "single_topic.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007205983726366147"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word least representative of this topic\n",
    "single_topic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004850248179647465"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word most representative of this topic\n",
    "single_topic[4197]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9789, 11782,  6417,  6415,  6712,  9354,  2949,   677,  2957,\n",
       "        7209])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 10 words for this topic:\n",
    "single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_indices = single_topic.argsort()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "science\n",
      "use\n",
      "learning\n",
      "learn\n",
      "machine\n",
      "researcher\n",
      "data\n",
      "algorithm\n",
      "datum\n",
      "model\n"
     ]
    }
   ],
   "source": [
    "for index in top_word_indices:\n",
    "    print(tfidf.get_feature_names()[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look like business articles perhaps... Let's confirm by using .transform() on our vectorized articles to attach a label number. But first, let's view all the 10 topics found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0007206  0.         0.         ... 0.         0.         0.00600381]\n",
      " [0.         0.00152527 0.         ... 0.         0.         0.00158674]\n",
      " [0.00593661 0.         0.00126903 ... 0.         0.00012436 0.        ]\n",
      " ...\n",
      " [0.         0.         0.0007598  ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(nmf_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TOP 15 WORDS FOR TOPIC #0\n",
      "['train', 'prediction', 'veeramachaneni', 'problem', 'paper', 'science', 'use', 'learning', 'learn', 'machine', 'researcher', 'data', 'algorithm', 'datum', 'model']\n",
      "[0.2717460201053615, 0.2776034423543002, 0.2814727603858789, 0.29804492437659585, 0.2981082070430484, 0.337891019680181, 0.4092908112061429, 0.41948146301344647, 0.4230123294562512, 0.5172362471896735, 0.5361625103603559, 0.5698032519378289, 0.6969971569325947, 0.8048838587210169, 1.1003275948509748]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #1\n",
      "['event', 'enterprise', 'security', 'attend', 'cyber', 'amsterdam', 'upcoming', 'london', 'cloud', 'blockchain', 'leader', 'industry', 'iot', 'locate', 'expo']\n",
      "[0.2459291715848743, 0.24711402397998056, 0.2599248977709651, 0.26281012772836526, 0.26286334833821423, 0.263802643948324, 0.26747678607118075, 0.27350408512461616, 0.2742712289584149, 0.28040142641791993, 0.2812799033479883, 0.28403736345255004, 0.306770774829742, 0.47565962855599225, 1.107956714637829]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #2\n",
      "['pichai', 'dean', 'deep', 'tensorflow', 'software', 'project', 'amazon', 'cloud', 'machine', 'learning', 'search', 'service', 'microsoft', 'company', 'google']\n",
      "[0.15842402800530944, 0.16048526768818033, 0.17998623852833945, 0.18811401206738795, 0.18835744807938265, 0.19068967324863462, 0.19629483053843735, 0.2063436234520314, 0.20649284046311708, 0.21495858189412798, 0.2814410769698133, 0.2824839379346238, 0.3484650616973689, 0.3747959563146591, 1.593297259324269]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #3\n",
      "['grandmaster', 'hui', 'hassabis', 'beat', 'machine', 'human', 'win', 'deepmind', 'player', 'match', 'lee', 'sedol', 'play', 'alphago', 'game']\n",
      "[0.15187489014026148, 0.16200350486674303, 0.16382760107712577, 0.1675079761705625, 0.16798080420176703, 0.16851067315250848, 0.2559497363349253, 0.3179441578522887, 0.3567368376998153, 0.43422070840910637, 0.43633207658889844, 0.4805932049285477, 0.543196151698721, 0.9528202655931158, 0.9909391551039236]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #4\n",
      "['work', 'project', 'director', 'collaboration', 'csail', 'computer', 'professor', 'engineering', 'ibm', 'research', 'student', 'lab', 'science', 'quest', 'mit']\n",
      "[0.1548381863352053, 0.1613522792921788, 0.17351400158789965, 0.19879947968726733, 0.23205393934147558, 0.2421096845460174, 0.26431574768979826, 0.2918334831276808, 0.29526455318897504, 0.32574671738645733, 0.32757454635611555, 0.3529443736459146, 0.3773735462167966, 0.421313217614457, 1.174599069806071]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #5\n",
      "['disinformation', 'use', 'pornographic', 'pelosi', 'medium', 'campaign', 'content', 'zuckerberg', 'news', 'face', 'election', 'clip', 'fake', 'video', 'deepfake']\n",
      "[0.15566399902951283, 0.15761424979438682, 0.1603407932670498, 0.16173715695003613, 0.16271977915074146, 0.16961480057388442, 0.17683965937331136, 0.18727144768914125, 0.22323070481157986, 0.22493501695721627, 0.23150919189096425, 0.290258716599574, 0.6371035569110186, 0.7821767463934504, 1.5581198972890973]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #6\n",
      "['pick', 'humanoid', 'control', 'algorithm', 'action', 'arm', 'environment', 'motion', 'object', 'csail', 'task', 'team', 'human', 'robotic', 'robot']\n",
      "[0.11367700735892217, 0.1183967691978277, 0.12762387770030817, 0.13030541187526018, 0.13049834526050264, 0.14106426002796105, 0.14160539917036483, 0.14921511016652172, 0.17277335392136384, 0.17550493028815597, 0.20634422354696275, 0.2189848672611703, 0.21996625266574668, 0.34042679120380775, 1.7413037226385926]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #7\n",
      "['record', 'diagnose', 'clinical', 'disease', 'risk', 'treatment', 'nhs', 'cancer', 'healthcare', 'hospital', 'care', 'medical', 'doctor', 'health', 'patient']\n",
      "[0.14139208353615257, 0.14495471872551535, 0.1977967345671109, 0.20564006383279332, 0.20929899197175153, 0.2314109389192268, 0.23422893782487833, 0.2838758114008752, 0.29095900270657943, 0.30221956625733687, 0.35184640529846123, 0.35426228562198075, 0.40850731309221977, 0.46772185238113767, 1.102677815411823]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #8\n",
      "['aclu', 'city', 'percent', 'ban', 'bias', 'enforcement', 'law', 'government', 'surveillance', 'face', 'use', 'technology', 'police', 'recognition', 'facial']\n",
      "[0.17258386028934752, 0.17504674487418462, 0.17722477757778912, 0.18292568782825527, 0.18668883963057925, 0.19766163364223807, 0.2070031848867318, 0.21501447779848804, 0.23915438964586375, 0.25845821963170673, 0.2733321363968662, 0.31194877501205925, 0.4726649397280534, 0.8022841699675446, 1.1258607745464169]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #9\n",
      "['run', 'build', 'new', 'device', 'tpu', 'power', 'market', 'design', 'company', 'hardware', 'processor', 'gpus', 'nvidia', 'intel', 'chip']\n",
      "[0.14960464668919862, 0.153827818005518, 0.15939724175888234, 0.16054726722058787, 0.16418082312654056, 0.17801637918106458, 0.17949709143147932, 0.19419594462958745, 0.19478509499229701, 0.22150490776209578, 0.23548157734993014, 0.28684911875192337, 0.4801692344086317, 0.5279758013368037, 1.5722458021537993]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #10\n",
      "['driverless', 'sensor', 'transportation', 'safe', 'driving', 'rus', 'map', 'traffic', 'self', 'autonomous', 'road', 'drive', 'driver', 'vehicle', 'car']\n",
      "[0.11193008901736899, 0.12475077350419975, 0.1345672149476227, 0.1447109681992498, 0.14743169520698757, 0.1564773474065241, 0.17608709532280994, 0.18099175803494474, 0.22844606228901346, 0.32984781124669205, 0.372633237289585, 0.38226823075187294, 0.43647331797285566, 0.5163782193335323, 0.834905104883202]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #11\n",
      "['like', 'people', 'business', 'company', 'amazon', 'voice', 'alexa', 'service', 'user', 'assistant', 'conversation', 'chatbot', 'customer', 'app', 'bot']\n",
      "[0.22125432083161425, 0.2274959718343101, 0.22955150883959421, 0.2318609229062254, 0.26157697361961535, 0.26753523424080333, 0.2687621426820303, 0.2806941833480235, 0.2829041846729696, 0.3140633435137374, 0.3201732396214759, 0.325771840378102, 0.4896529405143759, 0.49213213366532693, 0.7130431409651261]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #12\n",
      "['military', 'president', 'economic', 'american', 'report', 'chinese', 'company', 'uk', 'policy', 'worker', 'country', 'technology', 'government', 'job', 'china']\n",
      "[0.1765842336052278, 0.1805226312449289, 0.1809310883122516, 0.18204055580878242, 0.19279385338163338, 0.2073641062919072, 0.21275890038185166, 0.22784810701422512, 0.22990007104750046, 0.23522508733077716, 0.26730386674130796, 0.32548033169241347, 0.3979009451746687, 0.4851807469531892, 0.4934896791370012]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #13\n",
      "['980', '970', 'yu', 'camera', 'flagship', 'assistant', 'device', 'pro', 'feature', 'expo', 'npu', 'smartphone', 'kirin', 'mate', 'huawei']\n",
      "[0.12418068618617137, 0.12507476440726786, 0.12762690146001454, 0.1303090559989879, 0.14212081665067888, 0.14907730034659825, 0.15290970510367227, 0.160749581957164, 0.17288633630983546, 0.18526988315443874, 0.21844953354145807, 0.269762733351226, 0.2924889941817937, 0.37705120862232683, 1.4789267563914592]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #14\n",
      "['translation', 'technique', 'recognize', 'train', 'weight', 'researcher', 'learning', 'neuron', 'brain', 'node', 'deep', 'net', 'layer', 'neural', 'network']\n",
      "[0.15516967692509806, 0.1572559809914704, 0.17209722251569307, 0.1859228343013743, 0.18964531632294881, 0.20268279431358494, 0.25816760140496364, 0.2844447654945654, 0.30135506549511604, 0.321102635660464, 0.4149256625019078, 0.42542113651892394, 0.4371206201304682, 1.1237592480536052, 1.1316285698130935]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #15\n",
      "['write', 'know', 'turing', 'thing', 'like', 'program', 'people', 'question', 'test', 'ture', 'brain', 'think', 'computer', 'machine', 'human']\n",
      "[0.15098125352217173, 0.15128347428436756, 0.15229464430512182, 0.17297293868675795, 0.17644186514533333, 0.1809962642293598, 0.19262147226487003, 0.2016478961223106, 0.2157965203629558, 0.24234120987957597, 0.2752364855132569, 0.2790528069382667, 0.35434361644394746, 0.38172060737367264, 0.7580307909305728]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #16\n",
      "['speech', 'image', 'help', 'news', 'tool', 'source', 'open', 'hate', 'learning', 'content', 'lecun', 'deep', 'company', 'photo', 'facebook']\n",
      "[0.11722315953283606, 0.11764220406969546, 0.11787045685154748, 0.12308238649782742, 0.1285788347385937, 0.14427928126106654, 0.15623597343106876, 0.16349301019617465, 0.21679258487661757, 0.21923221276480556, 0.22740746988179153, 0.2536212072841086, 0.3054302882589603, 0.3179677354947626, 1.091123498399835]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student', 'working', 'new', 'social', 'discipline', 'science', 'department', 'chair', 'compute', 'group', 'faculty', 'schwarzman', 'computing', 'mit', 'college']\n",
      "[0.17691424668542943, 0.17896067382611092, 0.1858760372761101, 0.18600318891812948, 0.18710116235462412, 0.18883778649629557, 0.201111944456366, 0.20192655236087612, 0.21093706290546158, 0.3386646656925198, 0.3552239914419984, 0.36942881921884996, 0.4407141262894141, 0.5176877580231162, 0.7771627790351121]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #18\n",
      "['privacy', 'search', 'core', 'developer', 'federighi', 'device', 'app', 'machine', 'learning', 'salakhutdinov', 'ml', 'iphone', 'giannandrea', 'siri', 'apple']\n",
      "[0.09833276312372996, 0.09933415172268444, 0.09935400761494506, 0.11124034608417244, 0.11427688221683152, 0.12530945363353974, 0.12746857371338033, 0.13337767439800127, 0.14407404019209105, 0.15239834454038584, 0.15977794425144928, 0.1938160575895991, 0.25932051909887804, 0.29807708472063366, 1.0726292892115095]\n",
      "\n",
      "\n",
      "THE TOP 15 WORDS FOR TOPIC #19\n",
      "['use', 'sound', 'photo', 'train', 'computer', 'vision', 'pixel', 'frame', 'researcher', 'visual', 'scene', 'model', 'video', 'object', 'image']\n",
      "[0.13521460087667733, 0.15199871164677972, 0.15214125749439966, 0.15755345146173844, 0.16334211423370198, 0.16846706349820773, 0.16910131553246213, 0.17438133378295928, 0.19692623249565497, 0.20851065801764984, 0.22176883134918524, 0.23729589037004187, 0.24866765135526914, 0.4717889634929908, 0.8231736208385214]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,topic in enumerate(nmf_model.components_):\n",
    "    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    print([tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]])\n",
    "    print([topic[i] for i in topic.argsort()[-15:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for index,topic in enumerate(nmf_model.components_):\n",
    "#    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')\n",
    "    names = [tfidf.get_feature_names()[i] for i in topic.argsort()[-15:]]\n",
    "    weight = [topic[i] for i in topic.argsort()[-15:]]\n",
    "    d = {'Names' : names, 'Weight' : weight}\n",
    "    df = pd.DataFrame(d)\n",
    "    df = df.sort_values(by='Weight', ascending=False)\n",
    "    dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Names</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>model</td>\n",
       "      <td>1.100328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>datum</td>\n",
       "      <td>0.804884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>algorithm</td>\n",
       "      <td>0.696997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>data</td>\n",
       "      <td>0.569803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>researcher</td>\n",
       "      <td>0.536163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Names    Weight\n",
       "14       model  1.100328\n",
       "13       datum  0.804884\n",
       "12   algorithm  0.696997\n",
       "11        data  0.569803\n",
       "10  researcher  0.536163"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attaching Discovered Topic Labels to Original Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 10)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(407, 10)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform based on the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results_train = nmf_model.transform(dtm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1219, 20)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.01790269, 0.        , 0.00919475,\n",
       "       0.00296454, 0.00169547, 0.        , 0.02064694, 0.        ,\n",
       "       0.        , 0.15885794, 0.        , 0.0135937 , 0.00078248,\n",
       "       0.        , 0.        , 0.        , 0.04868635, 0.02181076])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.  , 0.  , 0.02, 0.  , 0.  ,\n",
       "       0.16, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.05, 0.02])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_train[0].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_train[0].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our model thinks that the first article belongs to topic #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining with Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_lemmatized_string</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10813</td>\n",
       "      <td>ZingBox aims for ‘Internet of Trusted Things’,...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>device\\niot\\nguardian\\napproach\\ndevices\\nindu...</td>\n",
       "      <td>Cybersecurity provider ZingBox has announced t...</td>\n",
       "      <td>cybersecurity provider zingbox has announced t...</td>\n",
       "      <td>[cybersecurity, provider, zingbox, have, annou...</td>\n",
       "      <td>cybersecurity provider zingbox have announce t...</td>\n",
       "      <td>[cybersecurity, provider, zingbox, announce, l...</td>\n",
       "      <td>cybersecurity provider zingbox announce launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10814</td>\n",
       "      <td>AI may help create more sustainable data centres</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...</td>\n",
       "      <td>Enterprise data centre provider Aegis Data arg...</td>\n",
       "      <td>enterprise data centre provider aegis data arg...</td>\n",
       "      <td>[enterprise, data, centre, provider, aegis, da...</td>\n",
       "      <td>enterprise data centre provider aegis data arg...</td>\n",
       "      <td>[enterprise, data, centre, provider, aegis, da...</td>\n",
       "      <td>enterprise data centre provider aegis data arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10815</td>\n",
       "      <td>Why a potential trillion dollar B2B bots indus...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "      <td>next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...</td>\n",
       "      <td>From Domino’s Pizza, to Uber, to Bank of Ameri...</td>\n",
       "      <td>from domino’s pizza to uber to bank of america...</td>\n",
       "      <td>[from, domino, ’s, pizza, to, uber, to, bank, ...</td>\n",
       "      <td>from domino ’s pizza to uber to bank of americ...</td>\n",
       "      <td>[domino, pizza, uber, bank, america, bots, hot...</td>\n",
       "      <td>domino pizza uber bank america bots hot proper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10816</td>\n",
       "      <td>Why companies investing in AI today should exp...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "      <td>ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...</td>\n",
       "      <td>Organisations investing in artificial intellig...</td>\n",
       "      <td>organisations investing in artificial intellig...</td>\n",
       "      <td>[organisation, invest, in, artificial, intelli...</td>\n",
       "      <td>organisation invest in artificial intelligence...</td>\n",
       "      <td>[organisation, invest, (, ), anticipate, 39, %...</td>\n",
       "      <td>organisation invest ( ) anticipate 39 % revenu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10817</td>\n",
       "      <td>Tencent gears up for greater GPU acceleration ...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "      <td>gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...</td>\n",
       "      <td>Tencent’s cloud computing services will be bee...</td>\n",
       "      <td>tencent’s cloud computing services will be bee...</td>\n",
       "      <td>[tencent, ’s, cloud, computing, service, will,...</td>\n",
       "      <td>tencent ’s cloud computing service will be bee...</td>\n",
       "      <td>[tencent, cloud, computing, service, beef, gpu...</td>\n",
       "      <td>tencent cloud computing service beef gpu accel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  10813  ZingBox aims for ‘Internet of Trusted Things’,...   \n",
       "1  10814   AI may help create more sustainable data centres   \n",
       "2  10815  Why a potential trillion dollar B2B bots indus...   \n",
       "3  10816  Why companies investing in AI today should exp...   \n",
       "4  10817  Tencent gears up for greater GPU acceleration ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
       "3  Organisations investing in artificial intellig...   \n",
       "4  Tencent’s cloud computing services will be bee...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  device\\niot\\nguardian\\napproach\\ndevices\\nindu...   \n",
       "1  data\\ncentre\\nnatural\\nnew\\ntechnology\\nindust...   \n",
       "2  next\\nbig\\ngupshup\\none\\nbusiness\\ntech\\nimpac...   \n",
       "3  ai\\norganisations\\nindustry\\nemployees\\nexpo\\n...   \n",
       "4  gpu\\naccelerators\\ngpus\\ncloud\\nservices\\ntesl...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cybersecurity provider ZingBox has announced t...   \n",
       "1  Enterprise data centre provider Aegis Data arg...   \n",
       "2  From Domino’s Pizza, to Uber, to Bank of Ameri...   \n",
       "3  Organisations investing in artificial intellig...   \n",
       "4  Tencent’s cloud computing services will be bee...   \n",
       "\n",
       "                                   text_preprocessed  \\\n",
       "0  cybersecurity provider zingbox has announced t...   \n",
       "1  enterprise data centre provider aegis data arg...   \n",
       "2  from domino’s pizza to uber to bank of america...   \n",
       "3  organisations investing in artificial intellig...   \n",
       "4  tencent’s cloud computing services will be bee...   \n",
       "\n",
       "                                     text_lemmatized  \\\n",
       "0  [cybersecurity, provider, zingbox, have, annou...   \n",
       "1  [enterprise, data, centre, provider, aegis, da...   \n",
       "2  [from, domino, ’s, pizza, to, uber, to, bank, ...   \n",
       "3  [organisation, invest, in, artificial, intelli...   \n",
       "4  [tencent, ’s, cloud, computing, service, will,...   \n",
       "\n",
       "                              text_lemmatized_string  \\\n",
       "0  cybersecurity provider zingbox have announce t...   \n",
       "1  enterprise data centre provider aegis data arg...   \n",
       "2  from domino ’s pizza to uber to bank of americ...   \n",
       "3  organisation invest in artificial intelligence...   \n",
       "4  tencent ’s cloud computing service will be bee...   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  [cybersecurity, provider, zingbox, announce, l...   \n",
       "1  [enterprise, data, centre, provider, aegis, da...   \n",
       "2  [domino, pizza, uber, bank, america, bots, hot...   \n",
       "3  [organisation, invest, (, ), anticipate, 39, %...   \n",
       "4  [tencent, cloud, computing, service, beef, gpu...   \n",
       "\n",
       "                                 text_cleaned_string  \n",
       "0  cybersecurity provider zingbox announce launch...  \n",
       "1  enterprise data centre provider aegis data arg...  \n",
       "2  domino pizza uber bank america bots hot proper...  \n",
       "3  organisation invest ( ) anticipate 39 % revenu...  \n",
       "4  tencent cloud computing service beef gpu accel...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>text_lemmatized_string</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>text_cleaned_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>12434</td>\n",
       "      <td>Robotic pets may be bad medicine for melancholy</td>\n",
       "      <td>Sherry Turkle finds human-machine love unsettling</td>\n",
       "      <td>Technology and society\\nResearch Laboratory of...</td>\n",
       "      <td>In the face of techno-doomsday punditry, Sherr...</td>\n",
       "      <td>in the face of techno-doomsday punditry sherry...</td>\n",
       "      <td>[in, the, face, of, techno, -, doomsday, pundi...</td>\n",
       "      <td>in the face of techno - doomsday punditry sher...</td>\n",
       "      <td>[face, techno, -, doomsday, punditry, sherry, ...</td>\n",
       "      <td>face techno - doomsday punditry sherry turkle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>12435</td>\n",
       "      <td>MIT develops Anklebot for stroke patients</td>\n",
       "      <td>Research team foresees robotic gym</td>\n",
       "      <td>arm\\ntrial\\ntherapy\\nclinical\\nhogan\\nmedical\\...</td>\n",
       "      <td>Clinical trials have already shown that an MIT...</td>\n",
       "      <td>clinical trials have already shown that an mit...</td>\n",
       "      <td>[clinical, trial, have, already, show, that, a...</td>\n",
       "      <td>clinical trial have already show that an mit r...</td>\n",
       "      <td>[clinical, trial, mit, robotic, arm, help, str...</td>\n",
       "      <td>clinical trial mit robotic arm help stroke pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>12436</td>\n",
       "      <td>Notes from the Lab</td>\n",
       "      <td>UNDERWATER ROBOTS THAT MAP AND NAVIGATE</td>\n",
       "      <td>Research Laboratory of Electronics\\nmit\\nImagi...</td>\n",
       "      <td>Imagine driving down an unfamiliar road and tr...</td>\n",
       "      <td>imagine driving down an unfamiliar road and tr...</td>\n",
       "      <td>[imagine, drive, down, an, unfamiliar, road, a...</td>\n",
       "      <td>imagine drive down an unfamiliar road and try ...</td>\n",
       "      <td>[imagine, drive, unfamiliar, road, try, find, ...</td>\n",
       "      <td>imagine drive unfamiliar road try find -PRON- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>12437</td>\n",
       "      <td>Ray and Maria Stata give $25 million to MIT</td>\n",
       "      <td>Gift is largest ever for Institute building pr...</td>\n",
       "      <td>together\\nnew\\nCampus buildings and architectu...</td>\n",
       "      <td>MIT today announced a $25 million donation by ...</td>\n",
       "      <td>mit today announced a $25 million donation by ...</td>\n",
       "      <td>[mit, today, announce, a, $, 25, million, dona...</td>\n",
       "      <td>mit today announce a $ 25 million donation by ...</td>\n",
       "      <td>[mit, today, announce, $, 25, million, donatio...</td>\n",
       "      <td>mit today announce $ 25 million donation ray m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625</th>\n",
       "      <td>12438</td>\n",
       "      <td>Reuters Uses AI To Prototype First Ever Automa...</td>\n",
       "      <td>AI is coming for journalism. But rather than s...</td>\n",
       "      <td>journalism\\ncontent generation\\nreuters\\nSynth...</td>\n",
       "      <td>AI is coming for journalism. But rather than s...</td>\n",
       "      <td>ai is coming for journalism but rather than si...</td>\n",
       "      <td>[ai, be, come, for, journalism, but, rather, t...</td>\n",
       "      <td>ai be come for journalism but rather than simp...</td>\n",
       "      <td>[come, journalism, simply, use, job, writer, r...</td>\n",
       "      <td>come journalism simply use job writer reuters ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "1621  12434    Robotic pets may be bad medicine for melancholy   \n",
       "1622  12435          MIT develops Anklebot for stroke patients   \n",
       "1623  12436                                 Notes from the Lab   \n",
       "1624  12437        Ray and Maria Stata give $25 million to MIT   \n",
       "1625  12438  Reuters Uses AI To Prototype First Ever Automa...   \n",
       "\n",
       "                                                summary  \\\n",
       "1621  Sherry Turkle finds human-machine love unsettling   \n",
       "1622                 Research team foresees robotic gym   \n",
       "1623            UNDERWATER ROBOTS THAT MAP AND NAVIGATE   \n",
       "1624  Gift is largest ever for Institute building pr...   \n",
       "1625  AI is coming for journalism. But rather than s...   \n",
       "\n",
       "                                                   tags  \\\n",
       "1621  Technology and society\\nResearch Laboratory of...   \n",
       "1622  arm\\ntrial\\ntherapy\\nclinical\\nhogan\\nmedical\\...   \n",
       "1623  Research Laboratory of Electronics\\nmit\\nImagi...   \n",
       "1624  together\\nnew\\nCampus buildings and architectu...   \n",
       "1625  journalism\\ncontent generation\\nreuters\\nSynth...   \n",
       "\n",
       "                                                   text  \\\n",
       "1621  In the face of techno-doomsday punditry, Sherr...   \n",
       "1622  Clinical trials have already shown that an MIT...   \n",
       "1623  Imagine driving down an unfamiliar road and tr...   \n",
       "1624  MIT today announced a $25 million donation by ...   \n",
       "1625  AI is coming for journalism. But rather than s...   \n",
       "\n",
       "                                      text_preprocessed  \\\n",
       "1621  in the face of techno-doomsday punditry sherry...   \n",
       "1622  clinical trials have already shown that an mit...   \n",
       "1623  imagine driving down an unfamiliar road and tr...   \n",
       "1624  mit today announced a $25 million donation by ...   \n",
       "1625  ai is coming for journalism but rather than si...   \n",
       "\n",
       "                                        text_lemmatized  \\\n",
       "1621  [in, the, face, of, techno, -, doomsday, pundi...   \n",
       "1622  [clinical, trial, have, already, show, that, a...   \n",
       "1623  [imagine, drive, down, an, unfamiliar, road, a...   \n",
       "1624  [mit, today, announce, a, $, 25, million, dona...   \n",
       "1625  [ai, be, come, for, journalism, but, rather, t...   \n",
       "\n",
       "                                 text_lemmatized_string  \\\n",
       "1621  in the face of techno - doomsday punditry sher...   \n",
       "1622  clinical trial have already show that an mit r...   \n",
       "1623  imagine drive down an unfamiliar road and try ...   \n",
       "1624  mit today announce a $ 25 million donation by ...   \n",
       "1625  ai be come for journalism but rather than simp...   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "1621  [face, techno, -, doomsday, punditry, sherry, ...   \n",
       "1622  [clinical, trial, mit, robotic, arm, help, str...   \n",
       "1623  [imagine, drive, unfamiliar, road, try, find, ...   \n",
       "1624  [mit, today, announce, $, 25, million, donatio...   \n",
       "1625  [come, journalism, simply, use, job, writer, r...   \n",
       "\n",
       "                                    text_cleaned_string  \n",
       "1621  face techno - doomsday punditry sherry turkle ...  \n",
       "1622  clinical trial mit robotic arm help stroke pat...  \n",
       "1623  imagine drive unfamiliar road try find -PRON- ...  \n",
       "1624  mit today announce $ 25 million donation ray m...  \n",
       "1625  come journalism simply use job writer reuters ...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 19, 19, ...,  0, 12,  7])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_train.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopheschellinck/anaconda3/envs/virtualenvchris/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "articles_train['Topic'] = topic_results_train.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                              title  \\\n",
      "727   11538  Amazon Wants Alexa to Hear Your Whispers and F...   \n",
      "1557  12370                     More-flexible machine learning   \n",
      "726   11537  Artificial Intelligence Has a Strange New Muse...   \n",
      "1236  12049  Bringing artificial intelligence and MIT to mi...   \n",
      "604   11415  Drag Queen vs. David Duke: Whose Tweets Are Mo...   \n",
      "...     ...                                                ...   \n",
      "831   11643  Google Is Giving Away AI That Can Build Your G...   \n",
      "584   11395  Poll Finds Americans Trust Police Use of Facia...   \n",
      "186   10998  Embracing the power of AI: The process behind ...   \n",
      "668   11477      What Trump’s Executive Order on AI Is Missing   \n",
      "331   11141  DeepMind’s first commercial product diagnoses ...   \n",
      "\n",
      "                                                summary  \\\n",
      "727   Amazon announced new listening features for Al...   \n",
      "1557  Giving machine-learning systems “partial credi...   \n",
      "726   The brain's way of processing smells is inspir...   \n",
      "1236  MIT researchers and collaborators have develop...   \n",
      "604   Opinion: Researchers used Google's AI tool to ...   \n",
      "...                                                 ...   \n",
      "831   The deep learning tool can identify all the sm...   \n",
      "584   The Pew Research Center reports 56% of America...   \n",
      "186   You want to start an AI project – but what pro...   \n",
      "668   Opinion: America needs a special visa program ...   \n",
      "331   DeepMind is preparing to launch its first comm...   \n",
      "\n",
      "                                                   tags  \\\n",
      "727   better\\nfeature\\nrecognizing\\nArtificial Intel...   \n",
      "1557  better\\nrhinoceros\\none\\nget\\nData\\nAlgorithms...   \n",
      "726   machine learning\\ntasks\\nolfactory\\nArtificial...   \n",
      "1236  three\\nSTEM education\\nopportunities\\nweek\\nbr...   \n",
      "604   levels\\nmaking\\nspeech\\nArtificial Intelligenc...   \n",
      "...                                                 ...   \n",
      "831   artificial intelligence\\nmachine learning\\ndee...   \n",
      "584   san\\nfrancisco\\nArtificial Intelligence\\nnew\\n...   \n",
      "186   machine\\nlearning\\napproaches\\nprocess\\nindust...   \n",
      "668   processing\\nprocess\\nArtificial Intelligence\\n...   \n",
      "331   patients\\nfirst\\nmedical\\ndeepmind\\nindustry\\n...   \n",
      "\n",
      "                                                   text  \\\n",
      "727   (Whispers) Amazon Alexa will soon notice if yo...   \n",
      "1557  Machine learning, which is the basis for most ...   \n",
      "726   Today’s artificial intelligence systems, inclu...   \n",
      "1236  In the age of Alexa, YouTube recommendations, ...   \n",
      "604   Social media platforms like Facebook, Twitter,...   \n",
      "...                                                 ...   \n",
      "831   Today, a teaspoon of spit and a hundred bucks ...   \n",
      "584   In May, San Francisco banned city agencies lik...   \n",
      "186   You want to start an AI project – but what pro...   \n",
      "668   President Trump signed an executive order on F...   \n",
      "331   DeepMind is preparing to launch its first comm...   \n",
      "\n",
      "                                      text_preprocessed  \\\n",
      "727   (whispers) amazon alexa will soon notice if yo...   \n",
      "1557  machine learning which is the basis for most c...   \n",
      "726   today’s artificial intelligence systems includ...   \n",
      "1236  in the age of alexa youtube recommendations an...   \n",
      "604   social media platforms like facebook twitter a...   \n",
      "...                                                 ...   \n",
      "831   today a teaspoon of spit and a hundred bucks i...   \n",
      "584   in may san francisco banned city agencies like...   \n",
      "186   you want to start an ai project – but what pro...   \n",
      "668   president trump signed an executive order on f...   \n",
      "331   deepmind is preparing to launch its first comm...   \n",
      "\n",
      "                                        text_lemmatized  \\\n",
      "727   [(, whispers, ), amazon, alexa, will, soon, no...   \n",
      "1557  [machine, learning, which, be, the, basis, for...   \n",
      "726   [today, ’s, artificial, intelligence, system, ...   \n",
      "1236  [in, the, age, of, alexa, youtube, recommendat...   \n",
      "604   [social, medium, platform, like, facebook, twi...   \n",
      "...                                                 ...   \n",
      "831   [today, a, teaspoon, of, spit, and, a, hundred...   \n",
      "584   [in, may, san, francisco, ban, city, agency, l...   \n",
      "186   [-PRON-, want, to, start, an, ai, project, –, ...   \n",
      "668   [president, trump, sign, an, executive, order,...   \n",
      "331   [deepmind, be, prepare, to, launch, -PRON-, fi...   \n",
      "\n",
      "                                 text_lemmatized_string  \\\n",
      "727   ( whispers ) amazon alexa will soon notice if ...   \n",
      "1557  machine learning which be the basis for most c...   \n",
      "726   today ’s artificial intelligence system includ...   \n",
      "1236  in the age of alexa youtube recommendation and...   \n",
      "604   social medium platform like facebook twitter a...   \n",
      "...                                                 ...   \n",
      "831   today a teaspoon of spit and a hundred buck be...   \n",
      "584   in may san francisco ban city agency like the ...   \n",
      "186   -PRON- want to start an ai project – but what ...   \n",
      "668   president trump sign an executive order on feb...   \n",
      "331   deepmind be prepare to launch -PRON- first com...   \n",
      "\n",
      "                                           text_cleaned  \\\n",
      "727   [(, whispers, ), amazon, alexa, soon, notice, ...   \n",
      "1557  [machine, learning, basis, commercial, -, syst...   \n",
      "726   [today, system, include, neural, network, broa...   \n",
      "1236  [age, alexa, youtube, recommendation, spotify,...   \n",
      "604   [social, medium, platform, like, facebook, twi...   \n",
      "...                                                 ...   \n",
      "831   [today, teaspoon, spit, buck, -PRON-, need, sn...   \n",
      "584   [san, francisco, ban, city, agency, like, poli...   \n",
      "186   [-PRON-, want, start, project, –, process, -PR...   \n",
      "668   [president, trump, sign, executive, order, feb...   \n",
      "331   [deepmind, prepare, launch, -PRON-, commercial...   \n",
      "\n",
      "                                    text_cleaned_string  Topic  \n",
      "727   ( whispers ) amazon alexa soon notice -PRON- t...     11  \n",
      "1557  machine learning basis commercial - system int...     19  \n",
      "726   today system include neural network broadly in...     19  \n",
      "1236  age alexa youtube recommendation spotify playl...     17  \n",
      "604   social medium platform like facebook twitter y...     16  \n",
      "...                                                 ...    ...  \n",
      "831   today teaspoon spit buck -PRON- need snapshot ...      2  \n",
      "584   san francisco ban city agency like police sher...      8  \n",
      "186   -PRON- want start project – process -PRON- nee...      0  \n",
      "668   president trump sign executive order february ...     12  \n",
      "331   deepmind prepare launch -PRON- commercial prod...      7  \n",
      "\n",
      "[1219 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(articles_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <p style=\"color:purple\">Step 6: Using the trained model to define the topics on an imported PDF-file </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step X: topic results with the PDF-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step X.a:  Importing PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note the capitalization\n",
    "import PyPDF2\n",
    "\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "import os\n",
    "# import str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christopheschellinck/Documents/Projects/project_NLP_humain/reports\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../reports')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_MGI = 'MGI-The-Age-of-Analytics-Full-report.pdf'\n",
    "url_DS_use_cases ='The Big Book of Data Science Use Cases.pdf'\n",
    "url_big_Data = 'Using big data to make better pricing decisions.pdf'\n",
    "url_AI_strat = 'aiadoptionstrategies-march2019pdf.pdf'\n",
    "url_main = 'main.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(url):\n",
    "    \n",
    "    f = open(url,'rb')\n",
    "    pdf_reader = PyPDF2.PdfFileReader(f)\n",
    "    \n",
    "    num_of_pages = pdf_reader.getNumPages()\n",
    "    \n",
    "    text = ''\n",
    "\n",
    "    for i in range(0, num_of_pages):\n",
    "#         text += \"Page Number: \" + str(i)\n",
    "#         text += \"- - - - - - - - - - - - - - - - - - - -\"\n",
    "        page_obj = pdf_reader.getPage(i)\n",
    "        text += page_obj.extractText()\n",
    "#         text += \"- - - - - - - - - - - - - - - - - - - -\"\n",
    "    # close the PDF file object\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text_from_pdf(url_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step X.b:  Applying transform on the test article string by TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hereunder in the original train alternative fi_transform is used but here transform only, otherwise the shape is no\n",
    "## corresponding with the topic_results of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"../message.txt\", \"r\") as f:\\n    \\n    single_article = f.read()\\n    #single_article = f\\n    \\nprint(single_article)\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open(\"../message.txt\", \"r\") as f:\n",
    "    \n",
    "    single_article = f.read()\n",
    "    #single_article = f\n",
    "    \n",
    "print(single_article)\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article = str(single_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm_test = tfidf.transform(articles_test['text_preprocessed'])\n",
    "# dtm_test = tfidf.transform(article).toarray()\n",
    "dtm_test = tfidf.transform([article])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12369)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## topics with the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform based on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results_test = nmf_model.transform(dtm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00557138, 0.00557025, 0.        , 0.00217062, 0.00417199,\n",
       "       0.        , 0.        , 0.0055254 , 0.00198606, 0.00354989,\n",
       "       0.00157003, 0.00333757, 0.00881008, 0.0147009 , 0.00170519,\n",
       "       0.00786329, 0.        , 0.        , 0.00372142, 0.00200447])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00557138 0.00557025 0.         0.00217062 0.00417199 0.\n",
      " 0.         0.0055254  0.00198606 0.00354989 0.00157003 0.00333757\n",
      " 0.00881008 0.0147009  0.00170519 0.00786329 0.         0.\n",
      " 0.00372142 0.00200447]\n"
     ]
    }
   ],
   "source": [
    "print(topic_results_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0056, 0.0056, 0.    , 0.0022, 0.0042, 0.    , 0.    , 0.0055,\n",
       "       0.002 , 0.0035, 0.0016, 0.0033, 0.0088, 0.0147, 0.0017, 0.0079,\n",
       "       0.    , 0.    , 0.0037, 0.002 ])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_test[0].round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the string of the file is most near topic number 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results_test[0].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12369)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
